{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuda import gpuarray\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ker = SourceModule(\"\"\" \n",
    "__global__ void mult_ker(float * array, int array_len)\n",
    "{\n",
    "     int thd = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "     int num_iters = array_len / blockDim.x;\n",
    "\n",
    "     for(int j=0; j < num_iters; j++)\n",
    "     {\n",
    "         int i = j * blockDim.x + thd;\n",
    "\n",
    "         for(int k = 0; k < 50; k++)\n",
    "         {\n",
    "              array[i] *= 2.0;\n",
    "              array[i] /= 2.0;\n",
    "         }\n",
    "     }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "mult_ker = ker.get_function(\"mult_ker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_len = 100*1024**2\n",
    "data = np.random.randn(array_len).astype('float32')\n",
    "data_gpu = gpuarray.to_gpu(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_event = drv.Event()\n",
    "end_event = drv.Event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has the kernel started yet? True\n",
      "Has the kernel ended yet? True\n"
     ]
    }
   ],
   "source": [
    "start_event.record()\n",
    "mult_ker(data_gpu, np.int32(array_len), block=(1024,1,1), grid=(1,1,1))\n",
    "end_event.record()\n",
    "end_event.synchronize()\n",
    "print('Has the kernel started yet? {}'.format(start_event.query()))\n",
    "print('Has the kernel ended yet? {}'.format(end_event.query()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has the kernel started yet? True\n",
      "Has the kernel ended yet? True\n"
     ]
    }
   ],
   "source": [
    "print('Has the kernel started yet? {}'.format(start_event.query()))\n",
    "print('Has the kernel ended yet? {}'.format(end_event.query()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel execution time in milliseconds: 178.464569 \n"
     ]
    }
   ],
   "source": [
    "print('Kernel execution time in milliseconds: %f ' % start_event.time_till(end_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events and streams\n",
    "\n",
    "We will now see how to use event objects with respect to streams; this will give us a highly intricate level of control over the flow of our various GPU operations, allowing us to know exactly how far each individual stream has progressed via the ```query``` function, and even allowing us to synchronize particular streams with the host while ignoring the other streams. \n",
    "\n",
    "First, though, we have to realize this—each stream has to have its own dedicated collection of event objects; multiple streams cannot share an event object. Let's see what this means exactly by modifying the prior example, ```multi_kernel_streams.py```. After the kernel definition, let's add two additional empty lists—`start_events` and `end_events`. We will fill these lists up with event objects, which will correspond to each stream that we have. This will allow us to time one GPU operation in each stream, since every GPU operation requires two events:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_len = 1024\n",
    "data = np.random.randn(array_len).astype('float32')\n",
    "data_gpu = gpuarray.to_gpu(data)\n",
    "out_gpu  = gpuarray.empty_like(data_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
