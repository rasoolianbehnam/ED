{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%maven org.nd4j:nd4j-cuda-9.0:1.0.0-beta\n",
    "%maven org.deeplearning4j:deeplearning4j-cuda-9.0:1.0.0-beta\n",
    "%maven org.datavec:datavec-api:1.0.0-beta\n",
    "\n",
    "%maven org.datavec:datavec-spark_2.11:1.0.0-beta_spark_1\n",
    "%maven org.deeplearning4j:dl4j-spark_2.11:1.0.0-beta_spark_1\n",
    "%maven org.knowm.xchart:xchart:3.5.2\n",
    "    \n",
    "%maven org.scala-lang:scala-library:2.13.0-M5-6e0cba7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.deeplearning4j.nn.graph.ComputationGraph;\n",
    "import org.deeplearning4j.nn.transferlearning.TransferLearning;\n",
    "import org.deeplearning4j.nn.api.OptimizationAlgorithm;\n",
    "import org.deeplearning4j.nn.weights.WeightInit;\n",
    "import org.deeplearning4j.nn.conf.*;\n",
    "import org.deeplearning4j.nn.conf.layers.*;\n",
    "import org.deeplearning4j.nn.conf.graph.rnn.*;\n",
    "import org.deeplearning4j.nn.conf.inputs.InputType;\n",
    "import org.deeplearning4j.nn.conf.WorkspaceMode;\n",
    "import org.deeplearning4j.optimize.listeners.ScoreIterationListener;\n",
    "import org.deeplearning4j.datasets.iterator.MultipleEpochsIterator;\n",
    "import org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator;\n",
    "import org.deeplearning4j.util.ModelSerializer;\n",
    ";\n",
    "import org.datavec.api.transform.*;\n",
    "import org.datavec.api.transform.transform.time.StringToTimeTransform;\n",
    "import org.datavec.api.transform.sequence.comparator.NumericalColumnComparator;\n",
    "import org.datavec.api.transform.transform.string.ConcatenateStringColumns;\n",
    "import org.datavec.api.transform.transform.doubletransform.MinMaxNormalizer;\n",
    "import org.datavec.api.transform.schema.Schema;\n",
    "import org.datavec.api.transform.metadata.StringMetaData;\n",
    "import org.datavec.api.records.reader.impl.csv.CSVRecordReader;\n",
    "import org.datavec.api.split.FileSplit;\n",
    "import org.datavec.spark.storage.SparkStorageUtils;\n",
    "import org.datavec.spark.transform.misc.StringToWritablesFunction;\n",
    "import org.datavec.spark.transform.SparkTransformExecutor;\n",
    "import org.datavec.api.transform.condition.*;\n",
    "import org.datavec.api.transform.condition.column.*;\n",
    "import org.datavec.api.transform.sequence.window.ReduceSequenceByWindowTransform;\n",
    "import org.datavec.api.transform.reduce.Reducer;\n",
    "import org.datavec.api.transform.reduce.AggregableColumnReduction;\n",
    "import org.datavec.api.transform.sequence.window.TimeWindowFunction;\n",
    "import org.datavec.api.transform.ops.IAggregableReduceOp;\n",
    "import org.datavec.api.transform.metadata.ColumnMetaData;\n",
    "import org.datavec.api.writable.*;\n",
    "import org.datavec.hadoop.records.reader.mapfile.MapFileSequenceRecordReader;\n",
    "//import org.datavec.api.util.ArchiveUtils;\n",
    "\n",
    "\n",
    "import org.nd4j.util.ArchiveUtils \n",
    "\n",
    "\n",
    ";\n",
    "import org.nd4j.linalg.api.ndarray.INDArray;\n",
    "import org.nd4j.linalg.dataset.api.MultiDataSetPreProcessor;\n",
    "import org.nd4j.linalg.dataset.api.MultiDataSet;\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions;\n",
    "import org.nd4j.linalg.activations.Activation;\n",
    "import org.nd4j.linalg.learning.config.*;\n",
    "import org.nd4j.linalg.factory.Nd4j;\n",
    "import org.nd4j.linalg.indexing.BooleanIndexing;\n",
    "import org.nd4j.linalg.indexing.INDArrayIndex;\n",
    "import org.nd4j.linalg.indexing.NDArrayIndex.*;\n",
    "import org.nd4j.linalg.indexing.conditions.Conditions;\n",
    ";\n",
    "import org.apache.spark.api.java.function.Function;\n",
    "import org.apache.commons.io.FileUtils;\n",
    "import org.joda.time.DateTimeZone;\n",
    "import org.joda.time.format.DateTimeFormat;\n",
    ";\n",
    "import scala.collection.JavaConversions.*;\n",
    "import scala.collection.JavaConverters.*;\n",
    "import scala.io.Source;\n",
    "import java.util.Random;\n",
    "import java.util.concurrent.TimeUnit;\n",
    "import java.io.*;\n",
    "import java.net.URL;\n",
    "\n",
    "File cache = new File(System.getProperty(\"user.home\"), \"/.deeplearning4j\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "public File downloadFile(File cache, String url) throws Exception {\n",
    "    String splitPath[] = url.split(\"/\");\n",
    "    String fileName = splitPath[splitPath.length-1];\n",
    "    System.out.println(fileName);\n",
    "    File tmpZip = new File(cache, fileName);\n",
    "    tmpZip.delete();\n",
    "    System.out.println(\"Downloading file...\");\n",
    "    FileUtils.copyURLToFile(new URL(url), tmpZip);\n",
    "    System.out.println(\"Finished Downloading.\");\n",
    "    return tmpZip;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already Exists.\n"
     ]
    }
   ],
   "source": [
    "File dataFile = new File(cache, \"/aisdk_20171001.csv\");\n",
    "if (!dataFile.exists()) {\n",
    "    String remote = \"http://blob.deeplearning4j.org/datasets/aisdk_20171001.csv.zip\";\n",
    "    File tmpZip = downloadFile(cache, remote);\n",
    "    System.out.println(\"Decompressing file...\");\n",
    "    ArchiveUtils.unzipFileTo(tmpZip.getAbsolutePath(), cache.getAbsolutePath());\n",
    "    tmpZip.delete();\n",
    "    System.out.println(\"Done.\");\n",
    "} else {\n",
    "    System.out.println(\"File already Exists.\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[IJava-executor-0] INFO org.apache.spark.SparkContext - Running Spark version 1.6.3\n",
      "[IJava-executor-0] WARN org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[IJava-executor-0] WARN org.apache.spark.util.Utils - Your hostname, phoenix resolves to a loopback address: 127.0.1.1; using 10.33.9.66 instead (on interface eno1)\n",
      "[IJava-executor-0] WARN org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "[IJava-executor-0] INFO org.apache.spark.SecurityManager - Changing view acls to: bzr0014\n",
      "[IJava-executor-0] INFO org.apache.spark.SecurityManager - Changing modify acls to: bzr0014\n",
      "[IJava-executor-0] INFO org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(bzr0014); users with modify permissions: Set(bzr0014)\n",
      "[IJava-executor-0] INFO io.netty.util.internal.PlatformDependent - Your platform does not provide complete low-level API for accessing direct buffers reliably. Unless explicitly requested, heap buffer will always be preferred to avoid potential system unstability.\n",
      "[IJava-executor-0] INFO org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38989.\n",
      "[sparkDriverActorSystem-akka.actor.default-dispatcher-3] INFO akka.event.slf4j.Slf4jLogger - Slf4jLogger started\n",
      "[sparkDriverActorSystem-akka.actor.default-dispatcher-3] INFO Remoting - Starting remoting\n",
      "[sparkDriverActorSystem-akka.actor.default-dispatcher-3] INFO Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.33.9.66:44579]\n",
      "[IJava-executor-0] INFO org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 44579.\n",
      "[IJava-executor-0] INFO org.apache.spark.SparkEnv - Registering MapOutputTracker\n",
      "[IJava-executor-0] INFO org.apache.spark.SparkEnv - Registering BlockManagerMaster\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-0d1e742c-7747-4fa6-95b2-4b6f4cb00894\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 3.4 GB\n",
      "[IJava-executor-0] INFO org.apache.spark.SparkEnv - Registering OutputCommitCoordinator\n",
      "[IJava-executor-0] INFO org.spark-project.jetty.server.Server - jetty-8.y.z-SNAPSHOT\n",
      "[IJava-executor-0] INFO org.spark-project.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040\n",
      "[IJava-executor-0] INFO org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.\n",
      "[IJava-executor-0] INFO org.apache.spark.ui.SparkUI - Started SparkUI at http://10.33.9.66:4040\n",
      "[IJava-executor-0] INFO org.apache.spark.executor.Executor - Starting executor ID driver on host localhost\n",
      "[IJava-executor-0] INFO org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43559.\n",
      "[IJava-executor-0] INFO org.apache.spark.network.netty.NettyBlockTransferService - Server created on 43559\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:43559 with 3.4 GB RAM, BlockManagerId(driver, localhost, 43559)\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.BlockManagerMaster - Registered BlockManager\n"
     ]
    }
   ],
   "source": [
    "/*Don't Run This part twice*/\n",
    "import org.apache.spark.SparkContext ;\n",
    "import org.apache.spark.sql.SQLContext ;\n",
    "import org.apache.spark.SparkConf ;\n",
    "\n",
    "SparkConf conf = new SparkConf().setAppName(\"T2P App\").setMaster(\"local[2]\");\n",
    "SQLContext sqlContext = new SQLContext(new SparkContext(conf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "EvalException",
     "evalue": "Failed to find data source: com.databricks.spark.csv. Please find packages at http://spark-packages.org",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1m\u001b[31mjava.lang.ClassNotFoundException: Failed to find data source: com.databricks.spark.csv. Please find packages at http://spark-packages.org\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.lookupDataSource(ResolvedDataSource.scala:77)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:102)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\u001b[0m",
      "\u001b[1m\u001b[31m\tat .(#109:1)\u001b[0m"
     ]
    }
   ],
   "source": [
    "sqlContext.read()\n",
    "    .format(\"com.databricks.spark.csv\")\n",
    "    .option(\"header\", \"true\") // Use first line of all files as header\n",
    "    .option(\"inferSchema\", \"true\") // Automatically infer data types\n",
    "    .load(dataFile.getAbsolutePath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".java",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.1+13-LTS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
