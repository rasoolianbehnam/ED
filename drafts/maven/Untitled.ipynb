{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%maven org.apache.spark:spark-core_2.11:2.4.0\n",
    "%maven org.datavec:datavec-spark_2.11:1.0.0-beta_spark_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkConf;\n",
    "import org.apache.spark.api.java.JavaRDD;\n",
    "import org.apache.spark.api.java.JavaSparkContext;\n",
    "import org.datavec.api.records.reader.impl.csv.CSVRecordReader;\n",
    "import org.datavec.api.transform.TransformProcess;\n",
    "import org.datavec.api.transform.schema.Schema;\n",
    "import org.datavec.api.writable.Writable;\n",
    "import org.datavec.spark.transform.SparkTransformExecutor;\n",
    "import org.datavec.spark.transform.misc.StringToWritablesFunction;\n",
    "import org.datavec.spark.transform.misc.WritablesToStringFunction;\n",
    "\n",
    "import java.util.Date;\n",
    "import java.util.List;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "int numLinesToSkip = 0;\n",
    "String delimieter = \",\";\n",
    "String baseDir = \"./\";\n",
    "String fileName = \"reports.csv\";\n",
    "String inputPath = baseDir + fileName;\n",
    "String timeStamp = String.valueOf(new Date().getTime());\n",
    "String outputPath = baseDir + \"reports_process_\" + timeStamp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Schema inputDataSchema = new Schema.Builder()\n",
    "    .addColumnsString(\"datetime\", \"severity\", \"location\", \"county\", \"state\")\n",
    "    .addColumnsDouble(\"lat\", \"lon\")\n",
    "    .addColumnsString(\"comment\")\n",
    "    .addColumnCategorical(\"type\", \"TOR\", \"WIND\", \"HAIL\")\n",
    "    .build();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformProcess top = new TransformProcess.Builder(inputDataSchema)\n",
    "    .removeColumns(\"datetime\", \"severity\", \"location\", \"county\", \"state\", \"comment\")\n",
    "    .categoricalToInteger(\"type\")\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====================\n",
      "--- Schema after step 0 (DataAction(RemoveColumnsTransform([datetime, severity, location, county, state, comment])))--\n",
      "Schema():\n",
      "idx   name        type           meta data\n",
      "0     \"lat\"       Double         DoubleMetaData(name=\"lat\",allowNaN=false,allowInfinite=false)\n",
      "1     \"lon\"       Double         DoubleMetaData(name=\"lon\",allowNaN=false,allowInfinite=false)\n",
      "2     \"type\"      Categorical    CategoricalMetaData(name=\"type\",stateNames=[\"WIND\",\"TOR\",\"HAIL\"])\n",
      "\n",
      "\n",
      "\n",
      "=====================\n",
      "--- Schema after step 1 (DataAction(CategoricalToIntegerTransform(columnName=type, columnIdx=2, stateNames=[TOR, WIND, HAIL], statesMap={WIND=1, HAIL=2, TOR=0})))--\n",
      "Schema():\n",
      "idx   name        type           meta data\n",
      "0     \"lat\"       Double         DoubleMetaData(name=\"lat\",allowNaN=false,allowInfinite=false)\n",
      "1     \"lon\"       Double         DoubleMetaData(name=\"lon\",allowNaN=false,allowInfinite=false)\n",
      "2     \"type\"      Integer        IntegerMetaData(name=\"type\",minAllowed=0,maxAllowed=2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "int numActions = top.getActionList().size();\n",
    "for (int i=0; i<numActions; i++) {\n",
    "    System.out.println(\"\\n\\n=====================\");\n",
    "    System.out.println(\"--- Schema after step \" + i + \" (\" + top.getActionList().get(i) + \")--\");\n",
    "    System.out.println(top.getSchemaAfterStep(i));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[IJava-executor-0] WARN org.apache.spark.util.Utils - Your hostname, phoenix resolves to a loopback address: 127.0.1.1; using 10.33.9.66 instead (on interface eno1)\n",
      "[IJava-executor-0] WARN org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "[IJava-executor-0] INFO org.apache.spark.SparkContext - Running Spark version 2.4.0\n",
      "[IJava-executor-0] WARN org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[IJava-executor-0] INFO org.apache.spark.SparkContext - Submitted application: Storm Reports Record Reader Transform\n",
      "[IJava-executor-0] INFO org.apache.spark.SecurityManager - Changing view acls to: bzr0014\n",
      "[IJava-executor-0] INFO org.apache.spark.SecurityManager - Changing modify acls to: bzr0014\n",
      "[IJava-executor-0] INFO org.apache.spark.SecurityManager - Changing view acls groups to: \n",
      "[IJava-executor-0] INFO org.apache.spark.SecurityManager - Changing modify acls groups to: \n",
      "[IJava-executor-0] INFO org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bzr0014); groups with view permissions: Set(); users  with modify permissions: Set(bzr0014); groups with modify permissions: Set()\n",
      "[IJava-executor-0] INFO org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34879.\n",
      "[IJava-executor-0] INFO org.apache.spark.SparkEnv - Registering MapOutputTracker\n",
      "[IJava-executor-0] INFO org.apache.spark.SparkEnv - Registering BlockManagerMaster\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-765e87ce-3651-470e-bbc6-f1a9fee0526c\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2.8 GB\n",
      "[IJava-executor-0] INFO org.apache.spark.SparkEnv - Registering OutputCommitCoordinator\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.util.log - Logging initialized @11772ms\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.Server - Started @11894ms\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@28b67479{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "[IJava-executor-0] INFO org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2031cf7e{/jobs,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a384930{/jobs/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a13f926{/jobs/job,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7475fca3{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@217ae1c2{/stages,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60466c79{/stages/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e1a33a9{/stages/stage,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10221939{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d61fcf0{/stages/pool,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77fe8817{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3557fbbb{/storage,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35bcc7f9{/storage/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77f83bc2{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74c5ab1a{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ec50c1a{/environment,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d959f34{/environment/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b85976f{/executors,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4241727e{/executors/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@143203f0{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15df427d{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22d69146{/static,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bcded10{/,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e0ccf3{/api,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a95cd5e{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64651330{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://10.33.9.66:4040\n",
      "[IJava-executor-0] INFO org.apache.spark.executor.Executor - Starting executor ID driver on host localhost\n",
      "[IJava-executor-0] INFO org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38947.\n",
      "[IJava-executor-0] INFO org.apache.spark.network.netty.NettyBlockTransferService - Server created on 10.33.9.66:38947\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 10.33.9.66, 38947, None)\n",
      "[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.33.9.66:38947 with 2.8 GB RAM, BlockManagerId(driver, 10.33.9.66, 38947, None)\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 10.33.9.66, 38947, None)\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 10.33.9.66, 38947, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[IJava-executor-0] INFO org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79636a8b{/metrics/json,null,AVAILABLE,@Spark}\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 107.2 KB, free 2.8 GB)\n",
      "[IJava-executor-0] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 2.8 GB)\n",
      "[dispatcher-event-loop-4] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 10.33.9.66:38947 (size: 20.4 KB, free: 2.8 GB)\n",
      "[IJava-executor-0] INFO org.apache.spark.SparkContext - Created broadcast 0 from textFile at $JShell$51.java:25\n"
     ]
    }
   ],
   "source": [
    "SparkConf sparkConf = new SparkConf();\n",
    "sparkConf.setMaster(\"local[*]\");\n",
    "sparkConf.setAppName(\"Storm Reports Record Reader Transform\");\n",
    "\n",
    "JavaSparkContext sc = new JavaSparkContext(sparkConf);\n",
    "\n",
    "JavaRDD<String> lines = sc.textFile(inputPath);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "JavaRDD<List<Writable>> stormReports = \n",
    "    lines.map(new StringToWritablesFunction(new CSVRecordReader()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "EvalException",
     "evalue": "class org.apache.hadoop.mapred.FileInputFormat tried to access method com.google.common.base.Stopwatch.<init>()V (org.apache.hadoop.mapred.FileInputFormat is in unnamed module of loader jdk.jshell.execution.DefaultLoaderDelegate$RemoteClassLoader @769e7ee8; com.google.common.base.Stopwatch is in unnamed module of loader 'app')",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1m\u001b[31mjava.lang.IllegalAccessError: class org.apache.hadoop.mapred.FileInputFormat tried to access method com.google.common.base.Stopwatch.<init>()V (org.apache.hadoop.mapred.FileInputFormat is in unnamed module of loader jdk.jshell.execution.DefaultLoaderDelegate$RemoteClassLoader @769e7ee8; com.google.common.base.Stopwatch is in unnamed module of loader 'app')\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:312)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\u001b[0m",
      "\u001b[1m\u001b[31m\tat scala.Option.getOrElse(Option.scala:121)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\u001b[0m",
      "\u001b[1m\u001b[31m\tat scala.Option.getOrElse(Option.scala:121)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\u001b[0m",
      "\u001b[1m\u001b[31m\tat scala.Option.getOrElse(Option.scala:121)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1378)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.rdd.RDD.first(RDD.scala:1377)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.api.java.JavaRDDLike$class.first(JavaRDDLike.scala:538)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.spark.api.java.AbstractJavaRDDLike.first(JavaRDDLike.scala:45)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.datavec.spark.transform.SparkTransformExecutor.execute(SparkTransformExecutor.java:169)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.datavec.spark.transform.SparkTransformExecutor.execute(SparkTransformExecutor.java:94)\u001b[0m",
      "\u001b[1m\u001b[31m\tat .(#53:1)\u001b[0m"
     ]
    }
   ],
   "source": [
    "JavaRDD<List<Writable>> processed = \n",
    "    SparkTransformExecutor.execute(stormReports, top);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".java",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.1+13-LTS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
